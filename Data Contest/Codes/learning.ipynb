{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "[MLENS] backend: threading\n"
     ]
    }
   ],
   "source": [
    "#importing the necessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datetime import datetime as dt\n",
    "from impyute.imputation.cs import mice\n",
    "from missingpy import MissForest\n",
    "from fancyimpute import IterativeImputer\n",
    "from mlens.ensemble import SuperLearner\n",
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"features_on_train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "for i in range(len(df)):\n",
    "    list.append(dt.strptime(df['last_rating_date'][i],\"%Y-%m-%d\"))\n",
    "df['last_rating_date']=np.array(list)\n",
    "\n",
    "list=[]\n",
    "for i in range(len(df)):\n",
    "    list.append(dt.strptime(df['first_rating_date'][i],\"%Y-%m-%d\"))\n",
    "df['first_rating_date']=np.array(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "for i in range(len(df)):\n",
    "    s=str(df['first_remark_date'][i])\n",
    "    if s != 'nan': list.append(dt.strptime(s,\"%Y-%m-%d\"))\n",
    "    else: list.append(np.nan)\n",
    "df['first_remark_date']=np.array(list)\n",
    "\n",
    "list=[]\n",
    "for i in range(len(df)):\n",
    "    s=str(df['last_remark_date'][i])\n",
    "    if s != 'nan': list.append(dt.strptime(s,\"%Y-%m-%d\"))\n",
    "    else: list.append(np.nan)\n",
    "df['last_remark_date']=np.array(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list=[]\n",
    "for i in range(len(df)):\n",
    "    d = df['last_rating_date'][i]-df['first_rating_date'][i]\n",
    "    list.append(d.days)\n",
    "df['diff1']=np.array(list)\n",
    "\n",
    "list=[]\n",
    "for i in range(len(df)):\n",
    "    d1 = df['last_remark_date'][i]\n",
    "    d2 = df['first_remark_date'][i]\n",
    "    if d1==np.nan or d2==np.nan : list.append(np.nan)\n",
    "    else :\n",
    "        d=d1-d2\n",
    "    list.append(d.days)\n",
    "df['diff2']=np.array(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = df['left']\n",
    "X = df.drop(columns=['last_rating_date','first_rating_date','last_remark_date','first_remark_date','left'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_ID = X['my_id']\n",
    "X = X.drop(columns=['my_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer=MissForest()  #imputing using miss forest and separating the test and train data set\n",
    "df1 = imputer.fit_transform(X)\n",
    "X = pd.DataFrame(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_x=StandardScaler()\n",
    "X_train=sc_x.fit_transform(X_train)\n",
    "X_test=sc_x.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Over_sampling\n",
    "\n",
    "smt=SMOTE()\n",
    "X_train,Y_train=smt.fit_sample(X_train,Y_train)\n",
    "# XGB classifier\n",
    "from xgboost import XGBClassifier\n",
    "classifier=XGBClassifier(n_estimators=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression Classifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier1=LogisticRegression(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#random forest classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "classifier2=RandomForestClassifier()\n",
    "#SVM\n",
    "from sklearn.svm import SVC\n",
    "#AdaBoost Classifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "adaboost=AdaBoostClassifier()\n",
    "#NaiveBayes \n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "naivebayes=GaussianNB()\n",
    "#ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Voting\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "model=VotingClassifier(estimators=[('lr',classifier1),('xgb',classifier),('xgb1',classifier),('xgb2',classifier),('rf',classifier2),('nb1',naivebayes),('rf1',classifier2),('ad1',adaboost),\n",
    "                                   ('xgb3',classifier),('xgb4',classifier)],voting='hard')\n",
    "model.fit(X_train,Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "y_test_pred=model.predict(X_test)\n",
    "cm=confusion_matrix(Y_test,y_test_pred)\n",
    "print(cm)\n",
    "a=cm[0][0]+cm[1][1]\n",
    "b=cm[0][0]+cm[0][1]+cm[1][0]+cm[1][1]\n",
    "print(a/b)\n",
    "\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "\n",
    "ans=balanced_accuracy_score(Y_test,y_test_pred)\n",
    "print(ans)\n",
    "\n",
    "f1=f1_score(Y_test,y_test_pred)\n",
    "print(f1,'f1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
